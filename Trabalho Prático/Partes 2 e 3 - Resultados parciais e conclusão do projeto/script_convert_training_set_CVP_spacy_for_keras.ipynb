{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import ast\n",
    "\n",
    "from gensim import corpora\n",
    "from six import iteritems\n",
    "from string import digits\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Realização da leitura do dataset que está no formato de entrada para o spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dataset = pd.read_csv('datasets/spacy_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento das StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n"
     ]
    }
   ],
   "source": [
    "arq = open(\"stopwords-pt.txt\", 'r')\n",
    "tokens = arq.readlines()\n",
    "stoplist = []\n",
    "for i in tokens:\n",
    "    stoplist.append(i.replace('\\n', ''))\n",
    "print(len(stoplist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Leitura do dicionário, produzido com base em todos os textos utilizados para o conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadDictionary(File):\n",
    "  with open(File, \"rb\") as myFile:\n",
    "      dict = pickle.load(myFile)\n",
    "      myFile.close()\n",
    "      return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = LoadDictionary('dic_ocorerencias.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remoção das StopWords do dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(393049 unique tokens: ['\"', '28/09/17,\"', '(daniele)', '(filho', '(fotografias),']...)\n"
     ]
    }
   ],
   "source": [
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Função responsável por fazer a limpeza do texto (remoção de ascentos, pontuação e etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    return normalize('NFKD', text.lower().translate(table)).encode('ASCII', 'ignore').decode('ASCII').translate(remove_digits).split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Função responsável por retornar um vector com os números que correspondem as palavras no dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dictionary(vector):\n",
    "    vector_numbers = []\n",
    "    for token in vector:\n",
    "        if dictionary.token2id.get(token) != None:\n",
    "            vector_numbers.append(dictionary.token2id.get(token))\n",
    "    return vector_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x(text):\n",
    "  vector_ints = find_dictionary(clean_string(text))\n",
    "  return vector_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_dataset = pd.DataFrame(columns=['X_train', 'Y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dataset[\"x_train\"] = spacy_dataset[\"text\"].apply(create_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. As duas proximas funções são responsáveis por construir o vector Y_train no formato de entrada aceito pelo Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate(_list, number):\n",
    "    return [element for element in _list for _ in range(number)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Definição da quantidade de classes que existem no conjunto de treino do Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLASSES = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector_y(vector_text):\n",
    "    vector = []\n",
    "    for _ in range(len(vector_text)):\n",
    "        aux = duplicate([0], NUMBER_OF_CLASSES)\n",
    "        aux.insert(0, 1)\n",
    "        vector.append(aux)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Construção de um objeto (dict) com as entidades e a frase ou string que está relacionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_entity_object(text, entities):\n",
    "    entity_object = {}\n",
    "    _string = \"\"\n",
    "    aux_ent = ast.literal_eval(entities)\n",
    "\n",
    "    for start, end, entity in aux_ent['entities']:\n",
    "        for pos, char in enumerate(text):\n",
    "            if pos >= start and pos <= end:\n",
    "                _string += char\n",
    "        entity_object[entity] = _string\n",
    "        _string = \"\"\n",
    "\n",
    "    return entity_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Construindo para cada entidade um vector com as strings que estão relacionadas com a entidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string_array_with_entity(entity_object):\n",
    "    for entity, text in entity_object.items():\n",
    "        entity_object.update({entity: clean_string(text)})\n",
    "\n",
    "    return entity_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Declaração de um objeto com todas as classes e seu vector correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_entity_object = {\n",
    "    'ADF': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'AB': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'HOM': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'FEM': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'INTER_POL': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'ACHAD_CAD': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "    'UNID_PRISIONAL': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'ACID_TRANSITO': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'LEG_DEFESA': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'MORADOR_RUA': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'DUPLO_HOM': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'LATROC': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'LEG_DEF_TER': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'MORTE_HOSP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'MORREU_DEPOIS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'EXECUCAO': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'CONFLITO_PM': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], \n",
    "    'FOLGA': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    'TRIPLO_HOM': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    'HOM_DOLOSO': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'LES_CORPORAL': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'LOC': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    'ORG': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    'PER': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "}\n",
    "# OUTHERS = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. A seguinte função faz o swap entre a entidade OTHERS e as outras entidades pré definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_entities(entity_object, vector_ints, y_vector):\n",
    "    pos_dic = []\n",
    "    pos_vector = []\n",
    "    \n",
    "    try:\n",
    "        for entity, vet_entity in entity_object.items():\n",
    "            for token in vet_entity:\n",
    "                if dictionary.token2id.get(token) != None:\n",
    "                    pos_dic.append(dictionary.token2id.get(token))\n",
    "\n",
    "            pos_vector = [vector_ints.index(i) for i in pos_dic]\n",
    "\n",
    "            for j in pos_vector:\n",
    "                y_vector[j] = default_entity_object[entity]\n",
    "            pos_dic = []\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return y_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>classif</th>\n",
       "      <th>x_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LESÕES: CINCO PERFURAÇÕES NA CABEÇA POR OBJETO...</td>\n",
       "      <td>{'entities': [(40, 64, 'AB'), (87, 91, 'PER'),...</td>\n",
       "      <td>[36678, 45703, 5191, 67792, 56409, 949, 1808, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A VÍTIMA FOI LESIONADA COM 5 PERFURAÇÕES A BAL...</td>\n",
       "      <td>{'entities': [(29, 47, 'ADF'), (130, 134, 'AB'...</td>\n",
       "      <td>[949, 2904, 11289, 189, 26122, 8494, 4791, 160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A VÍTIMA FOI ALVEJADA COM 07 LESÕES POR ARMA D...</td>\n",
       "      <td>{'entities': [(40, 52, 'ADF'), (445, 452, 'PER...</td>\n",
       "      <td>[949, 28253, 36678, 980, 1004, 33455, 1872, 89...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VÍTIMA DE HOMICÍDIO POR USO DE ARMA DE FOGO, 0...</td>\n",
       "      <td>{'entities': [(10, 19, 'HOM'), (31, 43, 'ADF')...</td>\n",
       "      <td>[949, 26119, 1117, 980, 1004, 36678, 28408, 45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>VÍTIMA COM 3 LESOES POR ARMA DE FOGO NA CABEÇA...</td>\n",
       "      <td>{'entities': [(24, 36, 'ADF')]}</td>\n",
       "      <td>[949, 36678, 980, 1004, 45703, 1655, 68493, 50...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  LESÕES: CINCO PERFURAÇÕES NA CABEÇA POR OBJETO...   \n",
       "1           1  A VÍTIMA FOI LESIONADA COM 5 PERFURAÇÕES A BAL...   \n",
       "2           2  A VÍTIMA FOI ALVEJADA COM 07 LESÕES POR ARMA D...   \n",
       "3           3  VÍTIMA DE HOMICÍDIO POR USO DE ARMA DE FOGO, 0...   \n",
       "4           4  VÍTIMA COM 3 LESOES POR ARMA DE FOGO NA CABEÇA...   \n",
       "\n",
       "                                             classif  \\\n",
       "0  {'entities': [(40, 64, 'AB'), (87, 91, 'PER'),...   \n",
       "1  {'entities': [(29, 47, 'ADF'), (130, 134, 'AB'...   \n",
       "2  {'entities': [(40, 52, 'ADF'), (445, 452, 'PER...   \n",
       "3  {'entities': [(10, 19, 'HOM'), (31, 43, 'ADF')...   \n",
       "4                    {'entities': [(24, 36, 'ADF')]}   \n",
       "\n",
       "                                             x_train  \n",
       "0  [36678, 45703, 5191, 67792, 56409, 949, 1808, ...  \n",
       "1  [949, 2904, 11289, 189, 26122, 8494, 4791, 160...  \n",
       "2  [949, 28253, 36678, 980, 1004, 33455, 1872, 89...  \n",
       "3  [949, 26119, 1117, 980, 1004, 36678, 28408, 45...  \n",
       "4  [949, 36678, 980, 1004, 45703, 1655, 68493, 50...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'X_train': '', 'Y_train': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in spacy_dataset.iterrows():\n",
    "#    print(row['classif', 'entities'])\n",
    "    vector_y_train = make_vector_y(row['x_train'])\n",
    "    dict_ = make_string_array_with_entity(make_entity_object(row[\"text\"], row[\"classif\"]))\n",
    "    \n",
    "    y_train = swap_entities(dict_, row['x_train'], vector_y_train)\n",
    " #   data['X_train'].append(row['x_train'])\n",
    " #   data['Y_train'].append(y_train)\n",
    "    keras_dataset = keras_dataset.append({'X_train': row['x_train'], 'Y_train': y_train}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36678,\n",
       " 45703,\n",
       " 5191,\n",
       " 67792,\n",
       " 56409,\n",
       " 949,\n",
       " 1808,\n",
       " 4176,\n",
       " 68493,\n",
       " 1655,\n",
       " 8274,\n",
       " 10601,\n",
       " 5385,\n",
       " 949,\n",
       " 9162,\n",
       " 4200,\n",
       " 11792]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_dataset['X_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_dataset['Y_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lesoes',\n",
       " 'cabeca',\n",
       " 'objeto',\n",
       " 'corto',\n",
       " 'contundente',\n",
       " 'vitima',\n",
       " 'conhecida',\n",
       " 'joao',\n",
       " 'informacoes',\n",
       " 'suspeito',\n",
       " 'agiu',\n",
       " 'legitima',\n",
       " 'defesa',\n",
       " 'vitima',\n",
       " 'envolveu',\n",
       " 'acidente',\n",
       " 'transito']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vet = []\n",
    "\n",
    "for i in keras_dataset['X_train'][0]:\n",
    "    vet.append(dictionary[i])\n",
    "vet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB',\n",
       " 'AB',\n",
       " 'AB',\n",
       " 'PER',\n",
       " 'LEG_DEFESA',\n",
       " 'LEG_DEFESA',\n",
       " 'ACID_TRANSITO',\n",
       " 'ACID_TRANSITO']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vet = []\n",
    "for i in keras_dataset['Y_train'][0]:\n",
    "    for key, value in default_entity_object.items():\n",
    "        if i == default_entity_object[key]:\n",
    "            vet.append(key)\n",
    "vet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ADF', 'AB', 'HOM', 'FEM', 'INTER_POL', 'ACHAD_CAD', 'UNID_PRISIONAL', 'ACID_TRANSITO', 'LEG_DEFESA', 'MORADOR_RUA', 'DUPLO_HOM', 'LATROC', 'LEG_DEF_TER', 'MORTE_HOSP', 'MORREU_DEPOIS', 'EXECUCAO', 'CONFLITO_PM', 'FOLGA', 'TRIPLO_HOM', 'HOM_DOLOSO', 'LES_CORPORAL', 'LOC', 'ORG', 'PER'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_entity_object.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_dataset.to_csv('keras_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
